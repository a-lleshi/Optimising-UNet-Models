{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test environment config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "import torch\n",
    "print(torch.__file__) \n",
    "print(torch.cuda.is_available())\n",
    "from torch.utils import collect_env\n",
    "print(collect_env.main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the environment has access to the NVIDIA A100 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Model - Lion Optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple implementation of the diffusion model in PyTorch without text decoder and encoder for a full text-to-image generation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision \n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms \n",
    "# from torchvision.transforms import Compose, ToTensor, Lambda, Resize, CenterCrop, RandomHorizontalFlip, ToPILImage\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Generates 150 samples of 25 columns x 10 rows of images\n",
    "def show(dataset, num_sample=150, cols=25, rows=10):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    for i, img in enumerate(dataset):\n",
    "        if i == num_sample:\n",
    "            break\n",
    "        plt.subplot(num_sample // rows + 1, cols, i + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img[0])\n",
    "\n",
    "# Download the dataset \n",
    "# *WARNING:* This will take a while to download (depending on connection speed) on the first ever run of this notebook\n",
    "data = torchvision.datasets.CelebA(root='', split=\"train\", download=True)\n",
    "\n",
    "# Show the first 150 samples\n",
    "show(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Forward Diffusion Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The linear schedule used in the forward diffusion process to calculate the alphas, betas, diffusion and posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# A linear schedule as proposed in https://arxiv.org/pdf/2102.09672.pdf\n",
    "def linear_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "# A cosine schedule as proposed in https://arxiv.org/abs/2102.09672.pdf\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "\n",
    "    return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "# A quadratic schedule\n",
    "def quadratic_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "\n",
    "    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2\n",
    "\n",
    "# A sigmoid schedule\n",
    "def sigmoid_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    betas = torch.linspace(-6, 6, timesteps)\n",
    "\n",
    "    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start\n",
    "\n",
    "# Returns a specific index t of a passed list of values vals while considering the batch dimension.\n",
    "def get_index_from_list(vals, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t.cpu())\n",
    "\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "# Returns the diffusion model's forward diffusion sample, taking an image x_0 and a timestep t as input and returning the noisy version.\n",
    "def forward_diffusion_sample(x_0, t, device=\"cpu\"):\n",
    "\n",
    "    noise = torch.randn_like(x_0)\n",
    "\n",
    "    sqrt_alphas_cumprod_t = get_index_from_list(sqrt_alphas_cumprod, t, x_0.shape)\n",
    "\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x_0.shape\n",
    "    )\n",
    "    \n",
    "    # mean + variance\n",
    "    return sqrt_alphas_cumprod_t.to(device) * x_0.to(device) \\\n",
    "    + sqrt_one_minus_alphas_cumprod_t.to(device) * noise.to(device), noise.to(device)\n",
    "\n",
    "\n",
    "# Define beta schedule\n",
    "T = 300\n",
    "betas = linear_beta_schedule(timesteps=T)\n",
    "\n",
    "# define alphas\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "\n",
    "# Calculate for diffusion q(x_t | x_{t-1})\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "\n",
    "# Calculate for posterior q(x_{t-1} | x_t, x_0)\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Preprocessing Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Parameters for the dataset with image size of 64x64, 128x128, 256x256 \n",
    "# These will be used to resize the images and test the models on different image sizes\n",
    "IMG_SIZE = 64\n",
    "IMG_SIZE_128 = 128\n",
    "IMG_SIZE_256 = 256\n",
    "\n",
    "# Batch size for training and testing with 128 images per batch and 256 images per batch.\n",
    "BATCH_SIZE = 128\n",
    "BATCH_SIZE_256 = 256\n",
    "\n",
    "# The tensor transformer for the dataset\n",
    "def load_transformed_dataset():\n",
    "    transform = transforms.Compose([ \n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda t: t * 2 - 1)\n",
    "    ])\n",
    "\n",
    "    data_transform = transform\n",
    "\n",
    "    train = datasets.CelebA(root='', split=\"train\", download=True, transform=data_transform)\n",
    "\n",
    "    test = datasets.CelebA(root='', split=\"test\", download=True, transform=data_transform)\n",
    "\n",
    "    return torch.utils.data.ConcatDataset([train, test])\n",
    "\n",
    "\n",
    "# Load the transformer dataset\n",
    "data = load_transformed_dataset()\n",
    "\n",
    "# Appends the data into a dataloader with a batch size of 128 or 256 depending on investigation\n",
    "dataloader = DataLoader(data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "# The reverse transformer for the dataset to show the images back to their original form\n",
    "def reverse_tensor_img(image):\n",
    "    reverse_transform = transforms.Compose([\n",
    "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
    "        transforms.Lambda(lambda t: t.permute(1, 2, 0)),\n",
    "        transforms.Lambda(lambda t: t*255),\n",
    "        transforms.Lambda(lambda t: t.cpu().numpy().astype(np.uint8)),\n",
    "        transforms.ToPILImage(),\n",
    "    ])\n",
    "\n",
    "    # Take first image of batch\n",
    "    if len(image.shape) == 4:\n",
    "        image = image[0, :, :, :]\n",
    "\n",
    "    plt.imshow(reverse_transform(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing if the forward diffusion process is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Convert reverse_tensor_img to a cuda tensor function\n",
    "# reverse_tensor_img = torch.jit.script(reverse_tensor_img)\n",
    "\n",
    "# Load a single image from the dataloader\n",
    "image = next(iter(dataloader))[0]\n",
    "\n",
    "# Add image dimensions for the graph, the amount of image steps and the step sizes\n",
    "plt.figure(figsize=(18, 18))\n",
    "plt.axis('off')\n",
    "num_images = 20\n",
    "stepsize = int(T/num_images)\n",
    "\n",
    "# Plot the image with the step size and show the image\n",
    "for idx in range(0, T, stepsize):\n",
    "    t = torch.Tensor([idx]).type(torch.int64)\n",
    "    plt.subplot(1, num_images+1, (idx//stepsize) + 1)\n",
    "    image, noise = forward_diffusion_sample(image, t)\n",
    "    plt.axis('off')\n",
    "    reverse_tensor_img(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Backward Diffusion Process (U-Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# The convolutional block for the model\n",
    "# The block consists of two convolutional layers with each one having its own batch normalization and a relu activation function\n",
    "# The block also has a time embedding layer that is used to add the time embedding to the convolutional layers\n",
    "# The block also has skip connections using the time embedding layer and the convolutional layers to add the time embedding to the skip connections\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        # Time embedding layer\n",
    "        self.time_mlp =  nn.Linear(time_emb_dim, out_channel)\n",
    "\n",
    "        # First convolutional layers\n",
    "        # If up is true then add a convolutional transpose layer to upsample the channels\n",
    "        if up:\n",
    "            self.conv1 = nn.Conv2d(2*in_channel, out_channel, 3, padding=1)\n",
    "            self.transform = nn.ConvTranspose2d(out_channel, out_channel, 4, 2, 1)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(in_channel, out_channel, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_channel, out_channel, 4, 2, 1)\n",
    "\n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel, 3, padding=1)\n",
    "\n",
    "        # Batch normalization layers for both convolutional layers\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_channel)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_channel)\n",
    "\n",
    "        # Relu activation function\n",
    "        self.relu  = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, t, ):\n",
    "        # First Conv\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        # Time embedding\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        # Extend last 2 dimensions\n",
    "        time_emb = time_emb[(..., ) + (None, ) * 2]\n",
    "        # Add time channel\n",
    "        h = h + time_emb\n",
    "        # Second Conv\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "        # Down or Upsample\n",
    "        return self.transform(h)\n",
    "\n",
    "# A sinusoidal time embedding layer as described in the paper https://arxiv.org/pdf/1706.03762.pdf\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "# A UNet architecture for the image denoising task with time embedding in each layer\n",
    "class SimpleUnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        image_channels = 3 # RGB: 3 channels for RED, GREEN, BLUE\n",
    "        down_channels = (64, 128, 256, 512, 1024) # Number of channels in each downsample layer\n",
    "        up_channels = (1024, 512, 256, 128, 64) # Number of channels in each upsample layer\n",
    "        out_dim = 1 # 1x1 final of output channels\n",
    "        time_emb_dim = 32 # Dimension of time embedding\n",
    "\n",
    "        # Time embedding\n",
    "        self.time_mlp = nn.Sequential(\n",
    "                SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "                nn.Linear(time_emb_dim, time_emb_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        # Initial projection\n",
    "        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1)\n",
    "\n",
    "        # Downsample\n",
    "        self.downs = nn.ModuleList([Block(down_channels[i], down_channels[i+1], \\\n",
    "                                    time_emb_dim) \\\n",
    "                    for i in range(len(down_channels)-1)])\n",
    "        # Upsample\n",
    "        self.ups = nn.ModuleList([Block(up_channels[i], up_channels[i+1], \\\n",
    "                                        time_emb_dim, up=True) \\\n",
    "                    for i in range(len(up_channels)-1)])\n",
    "\n",
    "        # Final output 1x1 conv\n",
    "        self.output = nn.Conv2d(up_channels[-1], 3, out_dim)\n",
    "\n",
    "    def forward(self, x, timestep):\n",
    "        # Embedd time\n",
    "        t = self.time_mlp(timestep)\n",
    "        # Initial conv\n",
    "        x = self.conv0(x)\n",
    "        # Unet\n",
    "        residual_inputs = []\n",
    "        for down in self.downs:\n",
    "            x = down(x, t)\n",
    "            residual_inputs.append(x)\n",
    "        for up in self.ups:\n",
    "            residual_x = residual_inputs.pop()\n",
    "            # Add residual x as additional channels\n",
    "            x = torch.cat((x, residual_x), dim=1)           \n",
    "            x = up(x, t)\n",
    "        return self.output(x)\n",
    "\n",
    "model = SimpleUnet()\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
    "model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Training, Lose, Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# A function to get the loss for the model given the input image and the timestep in PyTorch\n",
    "def get_loss(model, x_0, t, type=\"l1\"):\n",
    "    if type == \"l1\":\n",
    "        x_noisy, noise = forward_diffusion_sample(x_0, t, device)\n",
    "        noise_pred = model(x_noisy, t)\n",
    "        return F.l1_loss(noise, noise_pred)\n",
    "    elif type == \"l2\":\n",
    "        x_noisy, noise = forward_diffusion_sample(x_0, t, device)\n",
    "        noise_pred = model(x_noisy, t)\n",
    "        return F.mse_loss(noise, noise_pred)\n",
    "    else:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# A sample that calls the model to predict the noise in the image and returns the denoised image.\n",
    "# Applies noise to this image, if we are not in the last step yet.\n",
    "@torch.no_grad()\n",
    "def sample_timestep(x, t):\n",
    "    # Get noise from betas, timestep and image shape\n",
    "    betas_t = get_index_from_list(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = get_index_from_list(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
    "    )\n",
    "    sqrt_recip_alphas_t = get_index_from_list(sqrt_recip_alphas, t, x.shape)\n",
    "    \n",
    "    # Call model (current image - noise prediction)\n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "    posterior_variance_t = get_index_from_list(posterior_variance, t, x.shape)\n",
    "    \n",
    "    if t == 0:\n",
    "        return model_mean\n",
    "    else:\n",
    "        noise = torch.randn_like(x)\n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise \n",
    "\n",
    "# A function to plot the denoised image at each timestep showing a 10 step diffusion\n",
    "@torch.no_grad()\n",
    "def sample_plot_image():\n",
    "    # Sample noise\n",
    "    img_size = IMG_SIZE\n",
    "    img = torch.randn((1, 3, img_size, img_size), device=device)\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.axis('off')\n",
    "    num_images = 10\n",
    "    stepsize = int(T/num_images)\n",
    "\n",
    "    for i in range(0,T)[::-1]:\n",
    "        t = torch.full((1,), i, device=device, dtype=torch.long)\n",
    "        img = sample_timestep(img, t)\n",
    "        if i % stepsize == 0:\n",
    "            plt.subplot(1, num_images, int(i/stepsize+1))\n",
    "            reverse_tensor_img(img.detach().cpu())\n",
    "    plt.show()   \n",
    "\n",
    "# A function to return an np array of the denoised image at each timestep showing a 10 step diffusion for FID score\n",
    "@torch.no_grad()\n",
    "def sample_plot_FID():\n",
    "    # Sample noise\n",
    "    img_size = IMG_SIZE\n",
    "    img = torch.randn((1, 3, img_size, img_size), device=device)\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.axis('off')\n",
    "    num_images = 10\n",
    "    stepsize = int(T/num_images)\n",
    "\n",
    "    for i in range(0,T)[::-1]:\n",
    "        t = torch.full((1,), i, device=device, dtype=torch.long)\n",
    "        img = sample_timestep(img, t)\n",
    "        \n",
    "    # return an np array of the image\n",
    "    return img.detach().cpu().numpy()         \n",
    "\n",
    "# A function to plot our results for the model\n",
    "@torch.no_grad()\n",
    "def plot_results(results):\n",
    "\n",
    "    # Results is a list of tuples (loss, step) for each step\n",
    "    loss, step = zip(*results)\n",
    "    plt.plot(step, loss)\n",
    "    # loss_step = np.array(results)\n",
    "    # plt.plot(loss_step[:,1], loss_step[:,0])\n",
    "    plt.xlabel(\"Training Step\")\n",
    "    plt.ylabel(\"Training Loss\")\n",
    "    plt.title(\"Loss per step\")\n",
    "    plt.savefig(\"adam_loss.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"CUDA Avaliable: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Output the amount of parameters in the model and aviailable cuda devices\n",
    "print(\"Num params: \", sum(p.numel() for p in model.parameters()))\n",
    "print(\"Num devices: \", torch.cuda.device_count())\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Device CUDA capability: {torch.cuda.get_device_capability(0)}\")\n",
    "### Results\n",
    "# The number of prameters in the model is outputted.\n",
    "# The model is trained for 5 epochs at 1475 steps.\n",
    "# The model is trained on a single GPU (NVIDIA A100 40GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from lion_pytorch import Lion\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "optimiser = Lion(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "epochs = 4\n",
    "\n",
    "loss_step = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    print(f\"Amount of steps in dataloader: {len(dataloader)}\")\n",
    "    print(f\"Amount of batches in dataloader: {len(dataloader.dataset)}\")\n",
    "    print(f\"Batch size: {dataloader.batch_size}\")\n",
    "    running_loss = 0.0\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        t = torch.randint(0, T, (BATCH_SIZE,), device=device).long()\n",
    "        loss = get_loss(model, batch[0], t, \"l1\")\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        # fid_score(model, batch[0], t)\n",
    "        # Print the loss every 150 steps\n",
    "        if step % 10 == 0:\n",
    "            # fid_score(model, batch[0], t)\n",
    "            # Append the loss to a list with loss and step\n",
    "            loss_step.append([loss.item(), step])\n",
    "            # running_loss += loss.item() * \n",
    "            print(f\"Epoch {epoch} | step {step:03d} Loss: {loss.item()} \")\n",
    "\n",
    "        # Print the loss and image every 250 steps\n",
    "        if step % 250 == 0:\n",
    "            # fid_score(model, batch[0], t)\n",
    "\n",
    "            print(f\"Epoch {epoch} | step {step:03d} Loss: {loss.item()} \")\n",
    "            print(f\"Done with epoch {epoch} and step {step:03d}\")\n",
    "            # plot_results(loss_step)\n",
    "            sample_plot_image()\n",
    "\n",
    "\n",
    "        if step == 1427 or step == 1426 or step == 1428:\n",
    "            \"\"\" Final Output \"\"\"\n",
    "            print(f\"The final epoch is {epoch} and the final step is {step}\")\n",
    "            print(F\"The final loss is {loss.item()}\")\n",
    "\n",
    "            sample_plot_image()\n",
    "\n",
    "        # # Save loss and step to a csv file called adam_loss.csv\n",
    "        # with open(\"adam_loss.csv\", \"w\") as f:\n",
    "        #   writer = csv.writer(f)\n",
    "        #   writer.writerows(loss_step)\n",
    "\n",
    "        # # Make a plot from the loss and step\n",
    "        # plot_results(loss_step)\n",
    "    if epoch == 0:\n",
    "        print(loss_step)\n",
    "    if epoch == 1:\n",
    "        print(loss_step)\n",
    "    if epoch == 2:\n",
    "        print(loss_step)\n",
    "\n",
    "    # Once 100 epochs are done, save the model\n",
    "    if epoch == 3:\n",
    "        print(loss_step)\n",
    "        torch.save(model.state_dict(), \"model-lion.pt\")\n",
    "        print(\"Model saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
